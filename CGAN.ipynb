{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIBYC+RVXfeanb9SmQRkut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivyaMeenaSundaram/Deep_Learning_Lab/blob/main/CGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7vntCKF7UnP",
        "outputId": "eec57021-a884-40d5-8fa9-5c602020f42a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ðŸ“Œ STEP 1: Imports and Setup\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ STEP 2: Mount Drive and Transform\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awMzDBly7XID",
        "outputId": "3b180d8c-d8ee-47c4-8c28-f10643893a1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "root_dir = '/content/drive/MyDrive/Pneumonia dataset/chest_xray'\n",
        "\n",
        "for folder in os.listdir(root_dir):\n",
        "    print(f\"Folder: {folder}\")\n",
        "    folder_path = os.path.join(root_dir, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(\"Sample files:\", os.listdir(folder_path)[:5])\n",
        "    else:\n",
        "        print(\"Not a directory!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW1E9Ul58gia",
        "outputId": "735033d4-cbc8-44af-a087-f7a9ccd8776f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder: val\n",
            "Sample files: ['NORMAL', 'PNEUMONIA']\n",
            "Folder: train\n",
            "Sample files: ['PNEUMONIA', 'NORMAL']\n",
            "Folder: test\n",
            "Sample files: ['PNEUMONIA', 'NORMAL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Pneumonia dataset/chest_xray/train'\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "print(\"Classes:\", dataset.classes)\n",
        "print(\"Number of images:\", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbfdAv1d8pJJ",
        "outputId": "d52f076b-f283-40bf-b838-d0780db03045"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['NORMAL', 'PNEUMONIA']\n",
            "Number of images: 3610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChestXrayDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "        self.class_names = sorted(os.listdir(root_dir))  # read all folder names\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.class_names)}\n",
        "\n",
        "        for class_name in self.class_names:\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "            for file in os.listdir(class_dir):\n",
        "                if file.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "                    self.image_paths.append(os.path.join(class_dir, file))\n",
        "                    self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Pneumonia dataset/chest_xray/train'\n",
        "\n",
        "dataset = ChestXrayDataset(dataset_path, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "num_classes = len(dataset.class_names)\n",
        "print(\"Loaded dataset with\", len(dataset), \"samples\")\n",
        "print(\"Classes:\", dataset.class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4_mNA897cZx",
        "outputId": "11e487ff-f76c-482a-d6a7-5c43a38091bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 3610 samples\n",
            "Classes: ['NORMAL', 'PNEUMONIA']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ STEP 4: Conditional Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, img_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.label_embed = nn.Embedding(num_classes, z_dim)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, 256, 4, 1, 0, bias=False),  # 1x1 -> 4x4\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),     # 4x4 -> 8x8\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),      # 8x8 -> 16x16\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),       # 16x16 -> 32x32\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, img_channels, 4, 2, 1, bias=False),  # 32x32 -> 64x64\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_embed = self.label_embed(labels).unsqueeze(2).unsqueeze(3)  # [B, z_dim, 1, 1]\n",
        "        x = noise + label_embed  # Combine label and noise\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "AHVMlN-q7iq0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ STEP 5: Conditional Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.label_embed = nn.Embedding(num_classes, 64 * 64)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 1, 8),  # Output 1x1\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        label_map = self.label_embed(labels).view(labels.size(0), 1, 64, 64)\n",
        "        x = torch.cat([img, label_map], dim=1)  # Concatenate channel-wise\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "S1a2XY-j7liW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ STEP 6: Instantiate Models, Loss, Optimizers\n",
        "num_classes = 2  # set this before model instantiation\n",
        "z_dim = 100\n",
        "G = Generator(z_dim, img_channels=1, num_classes=num_classes).to(device)\n",
        "D = Discriminator(img_channels=1, num_classes=num_classes).to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "opt_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "opt_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(16, z_dim, 1, 1).to(device)\n",
        "fixed_labels = torch.tensor([i % num_classes for i in range(16)], device=device)\n"
      ],
      "metadata": {
        "id": "29JJBGzl7oNe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ STEP 7: Training Loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    for real_imgs, labels in dataloader:\n",
        "        real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
        "        fake_imgs = G(noise, labels)\n",
        "\n",
        "        D_real = D(real_imgs, labels).view(-1)\n",
        "        D_fake = D(fake_imgs.detach(), labels).view(-1)\n",
        "\n",
        "        real_targets = torch.ones_like(D_real)\n",
        "        fake_targets = torch.zeros_like(D_fake)\n",
        "\n",
        "        D_loss_real = criterion(D_real, real_targets)\n",
        "        D_loss_fake = criterion(D_fake, fake_targets)\n",
        "        D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "        opt_D.zero_grad()\n",
        "        D_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        output = D(fake_imgs, labels).view(-1)\n",
        "        G_loss = criterion(output, real_targets)\n",
        "\n",
        "        opt_G.zero_grad()\n",
        "        G_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "    # === Logging and Visualization ===\n",
        "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "        print(f\"[Epoch {epoch}] D_loss: {D_loss.item():.4f} | G_loss: {G_loss.item():.4f}\")\n",
        "        with torch.no_grad():\n",
        "            fake = G(fixed_noise, fixed_labels).detach().cpu()\n",
        "        grid = utils.make_grid(fake, nrow=4, normalize=True)\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(np.transpose(grid, (1, 2, 0)), cmap='gray')\n",
        "        plt.title(f\"Generated Images @ Epoch {epoch}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "7Tt6qGxH7uKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}